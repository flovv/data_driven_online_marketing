[
["index.html", "Data-Driven Marketing’s Darkest Secret - The pitfalls of buying targeting data Chapter 1 Motivation and Structure", " Data-Driven Marketing’s Darkest Secret - The pitfalls of buying targeting data Florian Teschner 2019-04-21 Chapter 1 Motivation and Structure We live in a connected world. No matter which website we visit, which app we use and which people we interact with: We leave a digital footprint. Day by day, there is more behavioral data created and it often makes using the internet more comfortable. Here is an example: Netflix concludes from user data which movies we like and subsequently optimizes which videos are suggested to us individually. Google individualizes search results and advertisers measure the effect of ad impressions on purchase probability. Tracking data helps companies to better understand consumer behavior and to customize their services. Moreover, it allows them to better target potential customers. This little booklet 1 tries to detail how tracking in web currently works, how advertisers use the tracking data to target consumers and finally analyses the question if targeting helps advertisers to improve their marketing ROI. The book is written for the data-driven marketeer, to show the benefits and drawbacks of the current practice. The book is organized as follows; first it details the basic mechanism of real-time advertising, followed by a more detailed view on how online targeting data is gathered. It tries to show that targeting data is rather crude in most cases and data providers have limited incentives to increase data quality. Taking the view of a marketeer, it analyses the effect of targeting on marketing ROI. Finally, it questions the current (mis-)use of tracking data in order to achieve higher sales and better ROI. Each section closes with a couple of questions marketeers might consider. A short note on the book title: The book itself is a small experiment on the Amazon marketplace. In digital content distribution it is commonplace to optimize headlines. This practice seems not to have taken hold in the book title generation so far. I used a simple web-service to test various multiple titles to increase “clicks”. As a result I got this catchy and click-bait title.↩ "],
["introduction.html", "Chapter 2 Introduction 2.1 Real-time Advertising", " Chapter 2 Introduction One of the key trends in the advertising industry is (digital) data-driven marketing. The whole thing starts with massive, passive data collection. No matter which website we visit or which app we use: We leave a digital footprint. These footprints are compiled for individual users and form so-called user profiles. A set of similar profiles is then aggregated to user segments, which aim at describing a homogeneous set of users with similar preferences and interests. In order to improve marketing activities, companies use these user segments to show them ads that fit the user’s interest. Before we dive into the details of data collection and profile generation, let’s have a simplified look at how ad space for individual users is bought. 2.1 Real-time Advertising If a single user visits a website which provides several advertisements slots, each ad placeholder calls a system called supplied side platform (SSP) and request a suitable ad. The request includes a user id (cookie) and the ad specification such as size, placement on the website and basic information about the website (Domain, Category, Content information). The supply side platform starts an auction to sell the ad placement to the advertiser with the highest bid. Advertisers change their bid price depending on the information they have about the user. For example, an insurance company might want to pay much more for users who are currently looking for a new insurance policy. To sum up, the technology allows to buy ad slots for individual users in real time. The industry calls this “buying a user”. The key question is how to value each individual user. In order to value individual users it is key to collect as much information (demographics, attitudes, personality, interests,…) as possible about each individual user. The better the information about a user, the better the advertiser can price the ad slot. Next, we detail how information gathering in these systems works. "],
["gathering-data.html", "Chapter 3 Gathering Data 3.1 The concept of pixels 3.2 How does the German tagging landscape look like? 3.3 Utilization of pixel data 3.4 Consequences for security &amp; e-commerce 3.5 Design of a deliberate data strategy", " Chapter 3 Gathering Data Day by day, there is more behavioral data created and it often makes using the internet more comfortable. Here is an example: Netflix concludes from user data which movies we like and subsequently optimizes which videos are suggested to us individually. Google individualizes search results and advertisers measure the effect of ad impressions on purchase probability. Tracking data helps companies to better understand consumer behavior and to customize their services. The following paragraphs detail how user data is collected and processed. 3.1 The concept of pixels User behavior on websites and platforms is mostly captured via tracking pixels (cookies). A “pixel” or “TAG” is a small piece of software that is loaded in the background of a website to collect information (undetected), about users and their behavior on the website. But sometimes these TAGs are forgotten and live on as tiny pieces of code somewhere in the depths of a website. In this case they keep collecting data - mostly unwittingly to the website owner. The control over these code snippets is often spread over multiple divisions and partners of a company (e.g. IT, website UX (creative agency), marketing (media agency) and customer relations (social media agency)). As a consequence, nobody is really responsible. But especially in this area it is important to know who is actually getting which information. Here is an example in the fashion industry: After the end of a joint campaign, an external data provider keeps collecting data about the website’s users (who are attractive for other brands) via the campaign TAG that was implemented to measure effects of the campaign. Without the website holder’s knowledge, these user profiles could be sold e.g. to a premium timepiece company or even to a competitor from the fashion industry to make a competitor’s campaign more efficient. The sad truth is that many companies neither know that many of these (external) code snippets still lie dormant on their websites nor for which party these TAGs collect data and which value this data could have for both the own company’s marketing and for other companies. Companies often lack a holistic, deliberately implemented data strategy, although answers to such questions are decisive for companies from many perspectives. 3.2 How does the German tagging landscape look like? In order to get an understanding of the current data collection landscape, we examined the 350 most important German websites. The analysis shows that the average German website has 16 external pixels implemented with the range being very wide reaching from almost 40 pixels in the tourism industry to just 2 in the pharmaceutical industry. As a general pattern we see that industries dealing with sensitive data (e.g. banks and insurances) tend to have fewer pixels on their websites. The distribution of single providers is interesting as well. Among analytics providers there is a clear order of use: Google Analytics can be found on 47% of all websites, followed by Adobe with Omniture (11%), Webtrekk (10%) and Piwik (4%) take 3rd and 4th position. 3.3 Utilization of pixel data Providers whose pixels/TAGs are implemented on multiple websites can track user behavior across the different websites to create detailed user profiles. These profile segments are very useful from the advertiser’s perspective because they enable them to address members of their target group in a favorable environment. But website operators are often left out when it comes to monetize the data. From the website operators’ perspective, it is therefore important to check whether data from external providers is used for such profiling and if so, how much their data is worth in this pool of profiles. Current rates for user profile data depend on the quality of the data and range from 1€to 5€ CPM based on impressions. For a website with a lot of traffic and detailed user profiles, the value of these data can add up to 100.000€ per month. 3.4 Consequences for security &amp; e-commerce Another problem occurs in the area of website security and user experience. As soon as unsafe pixels (http connection) are implemented into “safe” websites (https connection), the whole online traffic is no longer encoded. Some browsers (e.g. Internet Explorer) display security warnings in such cases. These security warnings interfere with the user and can lead to user aborting his/her online purchase. Thus, the implementation of unsafe TAGs has a direct effect on the user experience and thereby also an indirect effect on conversion rates or sales. Most TAGs are invocable as both https and http TAGs. 3.5 Design of a deliberate data strategy The first step is to get an overview of the owned platforms and to analyze the pixels that are used on them. The second step is the ongoing control of the implemented pixels and their regular check regarding necessity and use. So-called “TAG Management Systems” offered by Google and Adobe help to do that. These systems are especially useful to avoid time consuming agreements between IT and marketing. It is not surprising that such systems are widely spread already, they can be found on 35% of all websites. But companies using such systems are not fundamentally protected against mistakes. Illustrative view on implemented pixels It is the aim of such systems to administrate and control all external pixels at one place. But there are a couple of websites where pixels are implemented outside the “Tag Management System” and thereby undermine the idea of the system. For example, figure 4 shows that the Google Tag Manager is implemented, however many pixels are loaded directly from the main website. So, the arrows point from the actual website in purple directly to the pixels in blue. Considering the abundance and the complexity of the snares we have to ask why there are so many pixels still implemented and used. The benefit seems to exceed the costs by far. That mainly comes from a more efficient ad delivery because of carefully built user segments which can be addressed via targeting. On the consumer side, this segmentation usually happens on Data Management Platforms (DMP). In our survey, pixels of DMPs are found on 50% of the analyzed websites. Some DMPs also offer to advertisers to buy cookie data from third parties and to deliver ads on an individual basis. Questions an advertiser needs to consider: Which partners are implemented on my web properties? Are some of the implemented data partners using the data for other clients? Are all data partners mentioned in the data privacy agreement? Who in my organization checks and validates new partners before they are implemented on the web properties? "],
["owning-a-data-management-platform.html", "Chapter 4 Owning a Data Management Platform 4.1 DMP data integration 4.2 DMP technology integration 4.3 DMP media use cases", " Chapter 4 Owning a Data Management Platform Tracking data helps companies to better understand consumer behavior and to customize their services. Moreover, it allows them to better target potential customers. Similar words and slides are usually used to convince advertisers to join the pack, invest in their technology stack and buy a Data Management Platform (DMP). The following paragraphs detail what advertisers should consider buying into the promise of tracking and personalization. While no two advertisers are the same and the ROI depends on advertiser specific setups and parameters, there are four pillars to take into consideration; data and technology integration, potential use cases and finally cost. 4.1 DMP data integration User behavior on websites and platforms is mostly captured via tracking pixels (cookies). A “pixel” or “TAG” is a small piece of software that is loaded in the background of a website to collect information (undetected), about users and their behavior on the website. Providers whose pixels/TAGs are implemented on multiple websites can track user behavior across the diﬀerent websites to create detailed user proﬁles. These proﬁle segments are very useful from the advertiser’s perspective because they enable them to address members of their target group in a favorable environment. The key challenge with current cookie-based tracking is at least twofold. First, walled gardens such as facebook, youtube, amazon do not allow marketers to track their advertising. In general, the ability to track with DMP is very limited. This is a general trend; providers with valuable audiences will bundle data and inventory to avoid data leakage and monetize the audience on their properties. The second effect is public awareness. The Cambridge Analytica scandal increased the awareness for data privacy. Users ask for better privacy control and companies respond. Apple and Mozilla both improve the user’s possibility to opt out of any tracking. We assume that Google will follow in the next year by limiting 3rd party tracking in their browser and ad network. Finally, young, tech-informed audiences increasingly use ad and tracking blockers. To sum up, the data that DMPs will be able to collect in the future will be lacking key ad impressions, key audiences and be patchy at best. 4.2 DMP technology integration The other key ingredient is technology integration. The insights and audience data collected in the DMP needs to be connected to systems that change the user experience. These are basically three systems; Demand Side Platforms (DSP) buying individual ad impressions for users in certain target audiences, ad and onsite personalization engines with change the message and advertising content depending on the target audience. The systems need to be connected and work seamless together. Currently this is not the case as there are no industry standards. Hence advertisers need to carefully check which technology works well with other system in the ad/martech ecosystem. Besides the pure connectivity there are two quality metrics that are important to consider; speed and match-rate. Speed describes how often the systems are synced and how fast user profiles are transferred between the systems. Currently, it can take up to 24 hours for a user profile to be transferred from the DMP to DSP. In a retargeting use case; most advertisers are better off using DSP retargeting pixel with no time delay. Finally, the metric match-rate describes how many users profiled gathered in the DMP can be used in the activation. The rates vary by country and technology combination but are usually in the range of 70-80%. 4.3 DMP media use cases Most use cases focus on three areas: Targeting Costs, Frequency Management, Personalization. Potential use cases for a ROI positive DMP implementation. While the use cases can be generally be solved with the DMP; the key question is: “are there alternative solutions?” and what is the gain/lift by using the DMP versus using the alternative. In Germany, targeting cost (on display and video inventory) is less than 5% on average. Key publishers such as facebook and google provide targeting data without additional costs. One key asset DMPs claim to have exclusively is retargeting, segmentation and lookalike audiences. All these aspects can be (better) solved with the current DSP solutions. Frequency is traditionally measured in the adserver. As there is currently no link from the adserver to the buying systems (DSPs); overall frequency management is an issue if the advertiser uses multiple DSPs at the same time. Currently most advertisers in Germany follow a single DSP strategy and can use their DSP to control frequency and overexposure. Ad and Onsite Personalization: Ad personalization focuses mainly on retargeting segments and showing these segments the last (most relevant) offer. This case is due to the DSP speed advantage better served with the direct DSP integration. If the knowledge of the last viewed offer is transferred to the ad system with a delay of 24 hours, the potential advantage of segmentation and better prediction is minuscules. ## DMP cost considerations There are three cost aspects to consider: Implementation, maintenance, and license. DMP implementations are time-consuming and cost intensive. We have seen consulting companies working on an implementation for 4 months with 3 FTEs. The license cost of well-know DMP brands is around 100-200K per year. The maintenance is far from automated. Tags need to be managed, campaigns to be trafficked multiple times and clients need to build a strategy and new workflow around the technology. All in all, DMPs are not a plug and play solutions but rather a complex, cost and time-consuming project. The sales slides of DMP vendors promise the world of complete tracking and real-time personalization. The German reality is more complex. As a rough guideline; even clients with substantial digital investments in open market video and display inventory will have a hard time to turn a DMP project into a ROI positive endeavor with media cases only. Questions an advertiser needs to consider: - Is there a positive ROI in DMP projects? - What is the total cost of ownership? - What are the touchpoints that can reliably be measured? - Do you have user groups which have excessive ad exposure and can this be reduced? - Which of the use cases can only be handled by a DMP? "],
["modelling-user-profiles.html", "Chapter 5 Modelling User Profiles 5.1 What is look-a-like modelling? 5.2 Factors influencing modelling accuracy", " Chapter 5 Modelling User Profiles The whole thing starts with massive, passive data collection. No matter which website we visit or which app we use: We leave a digital footprint. These footprints are compiled for individual users and form so-called user profiles. A set of similar profiles is then aggregated to user segments, which aim at describing a homogeneous set of users with similar preferences and interests. 5.1 What is look-a-like modelling? Lookalike modelling is “finding new people who behave like current customers – or like a defined segment. The topic is hot, but there’s deliberate mystery about the method and its accuracy” 2 The following section, will demystify the method and bring some science to it. The basic idea is to find users who belong to a defined audience. Starting from a seed audience such as users who already converted or users that have state to belong to an audience through an survey, the idea is to train a statistical model to identify which user actions separate that user cluster from other users. Let’s have a look at a simple example to predict a user’s gender. The data usually used to create the lookalike models consists of the websites a user has visited in the past and one outcome variable. In this example that outcome variable is the user’s gender. Schematic view of the look-a-like prediction process That information is fed into various algorithms to train models on the data. One such model can be a tree-based model. The following chart exemplifies such a “Tree” model. As basis we used 20000 users and just 4 websites to classify users depending on their gender. The websites are illustrated as boxes in the middle. In the first step the model tells us that if a user visited chip.de (N&gt;=1) he is with a probability of 65% male. As a more complex example down the tree is if a user did not visit chip.de but gesuende.de she is with 73% female. That tree gets more complex if users can be segmented using more websites. Example tree-algorithm to decide on user attribute Next, we discuss several factors that influence how well lookalike modelling approaches work. 5.2 Factors influencing modelling accuracy Model accuracy describes how well the model classifies users. As a simple measure, we take the prediction accuracy defined as the number of correctly classified users as a percentage of all classified users. For the previous example of predicting a user’s gender we see that the accuracy is a function of how many websites are used. In the example chart 62% of all users are correctly classified by just using 20 randomly selected websites as data source. That number increases to roughly 73% when the model has access to 500+ websites. After that, additional websites do not add predictive value. Prediction accuracy is dependent on website universe This means that data providers theoretical prediction quality depends on number of websites their tracking code is implemented on. Small scale data providers with a small “tracking universe” will very likely have a hard time to use lookalike modelling approaches or conversely their prediction accuracy will be subpar. Another important aspect is the so-call prediction threshold. Machine learning algorithms usually output a probability that a user for each user classification. For example, the predicting a user’s gender might result in a prediction 80% male, 20% female. In a second step, the system applies a threshold to label the user. For users where the output is uncertain e.g. 50% male/female it depends on the system setting if this user’s gender is labeled “unknown”. The threshold setting introduces a moral challenge for the data provider. Increasing the threshold increases the prediction accuracy. Only users are classified for which the algorithm is certain that the prediction is correct. However, setting the threshold higher, reduces the number of users classified. Hence, the scale which the data provider aims to sell. The following chart exemplifies the relationship. Prediction accuracy is dependent on the share of users classified In the dataset studied, we see that if we aim for a prediction accuracy of 80% for gender, we can only classify 20% of all users. This relationship depends on the data provider’s scale (see previous point) as well as the classification (gender is different than age brackets). In general, a models prediction accuracy depends on the classification type. There are two simple reasons for that; a) an algorithmic and b) an informational reason. For the algorithmic reason, some classification types are harder than others. The gender classification is binary (e.g. coin flip). Hence the base rate is 50%. If the algorithm should predict age brackets there are multiple options to pick (20-30 vs 30-40 vs 40-50, …). The base-rate is consequential lower which means that an algorithm predicting at random will have a worse prediction accuracy in the case of the age prediction. The informational aspect touches the fundamentals of lookalike modelling; there needs to be information in the features used. Users which belong to different classes need to have distinctively different website usage. If users belong to different classes but have the same behavior the model cannot successfully differentiate between them. The prediction accuracy will suffer. To stick with the age bracket example; users age 39 have a very similar website usage behavior compared to users age 41. One point that is often mentioned by data providers is their superior machine learning / artificial intelligence / deep learning algorithm. While there might be cases (natural language processing or computer vision) where advanced AI have an edge, in most lookalike scenarios this is not the case. The data described in the last paragraphs is pretty clean and can be easily handled by off the shelf algorithms (e.g. XGBoost, random forest). The AI label is often used in “sales” pitches to deflect from the really important questions; scale of tracking, threshold to predict and accuracy of different classification types. Questions that advertisers should ask providers of lookalike models: How accurate are Lookalike models? What are the key factors? What is the effect of the number of websites used to measure user behavior? What is the effect of different Machine Learning algorithms? What is the effect the selection of websites used to measure user behavior? What is the minimum number of users needed to create models? Which attributes can reliably been used to create models? https://www.theguardian.com/media-network/media-network-blog/2013/sep/06/lookalike-modelling-advertising-demystified↩ "],
["buying-targeting-data.html", "Chapter 6 Buying Targeting Data 6.1 On intent data 6.2 Selecting data providers 6.3 Evaluating targeting data", " Chapter 6 Buying Targeting Data In the last chapter, we discussed how data is gathered, processed and user interest/demograhics are predicted. The process depends on multiple elements: Tracking Technology - measuring user interactions across the web Prediction Technology - using machine learning to segment profiles and Auction Technology - buying ad slots for individual users. To setup, run and combine these systems is costly, error-prone and time consuming. Hence, it is very common for advertisers to rely on third party data providers. In most cases, the standard data buying from data providers is facilitated through the trading interface. Data providers list all their user segments (e.g. car insurance seeker, or demographic segments such as male 25-30). The list also includes the pricing for each segment. Usually, prices are on a cost per mille basis and range from 10 cents to 10$ per 1000 impressions. On the buy-side, the advertiser selects fitting segments from one or multiple data providers through an interface. In the following paragraph, we focus on three major targeting data categories; demographic, interest and intent data. The first category is important as it is used to reach broadly defined target groups (e.g. male 20-49), which are commonly used in offline media. Interest data is often used as a well correlated proxy for long term buying probability or to match the advertising creative to the user interest. For example, interest in golf is highly correlated with high income, status-oriented clientele. Showing ads related to automotive, luxury watches or simply golf equipment seems like a good fit. Finally, intent data assumes that users go through a buying decision journey. Let’s take the example of a car insurance; a user might first read articles about car insurance in general and then go to comparison web sites and finally search for a specific insurance website. Such a user would be called “in-market” for and insurance. In theory, intent data selects users which have not yet bought a certain product and are still receptive to advertising in this category. 6.1 On intent data While profiles in the first two targeting data categories are relatively stable over time. Intent data has a very obvious and relevant time aspect. Users join and leave this category on a constant basis. This is especially important as customer decision journeys are very heterogenous. They vary between product categories as well as between users. Some users are quick to decide while others research potential products for long time spans. Data providers usually provide segments for individual product categories (e.g. car insurance). The users within the segment are binary labeled as “in-market” for the product category. The question arises when a data provider or the underlying algorithm decides that a user is “in-market” and when it decides that the same user fulfilled the buying intent. The following chart illustrates the process. A user starts with a base probability of buying a certain item or service. Buying intent vs in-market categorization She visits multiple websites to form an opinion about the available options. These visits are (partly) tracked by data providers. Hence, they identify an increase in buying probability and eventually label the user as “in-market”. Advertisers can now buy this user and show relevant ads. Here the first problem arises; inherently there is a time delay between the (first) user actions and the algorithms labeling as in-market. Depending on the algorithmic update rate and the data providers integrations with websites and buying systems these delays can be substantial (multiple days). Advertisers however would ideally show ads to users before they formed an opinion. Let’s assume that the user has finally bought the product. Here the second problem arises. In most cases it is very unlikely that the data providers are able to track the sales. This is due to two reasons; a) still most sales happen offline even though the information phase happens primarily online. b) most ecommerce and corporate sites do not implement pixels from data providers in order not to leak information to competitors. Hence, data providers need arbitrary rules to remove the “in-market” label from users. As advertisers continue to advertise to users in the “in-market” segment, users are irritated by continuous advertisement of products they already have bought. Questions advertisers need to ask about intent data: How long is the decision journey for my product? What is the variance in the decision journey length? Is advertising in the late stages of the decision journey sensible? When is a user labeled as “in-market”? (Which interactions are indicative?) How long does the data provider need to deliver the data from the indicative actions till the user is available in the buying system? How are users who bought the product removed from the system? These questions might help to pick the right intent data provider. The following paragraphs detail further aspects. 6.2 Selecting data providers When running an advertisement campaign, advertisers usually try to reach as many individuals of their target group as possible. As a response data providers and publishers routinely communicate their reach figures for key target groups (e.g. male 20-49 or young adults 18-30). These figures need to be taken with caution. In 2017 Facebook claimed that its platform allows advertisers to reach 1.7 million more 15- to 39-year-old users in Australia than the country’s official population.3 However, in order to decide which data providers to use as an advertiser, it is very illustrating to create the following chart. One the one axis we plot data provider’s reach figure, on the y-axis we plot the expected data quality for the attribute in question. Again, data quality refers to how correctly users are classified in terms of a certain attribute. The following example shows reach and data for demographic data. We expect platforms with login data (facebook and miles&amp;more) to deliver a high share of impression within the target group. In contrast, publisher and other data provider need to rely on lookalike approaches and sequentially will have a certain share of impression delivered outside the target group. Classifying data providers by reach and quality (Demographics) This chart needs to be created for every target group criteria separately. If we look a data quality for intent data the picture changes. Google knows pretty well what people have researched in the past and are likely to buy in future, whereas facebook has limited insights into immediate buying intent. Classifying data providers by reach and quality (Intent) Mapping different data providers gives a good understanding of their potential. 6.2.1 Targeting data and competition As we will discuss in the following paragraph evaluating data providers is very hard. For very common segments such as demographic or interest data advertisers need to consider two things when buying data; data accuracy and pricing. In general, using data providers increases the cost to reach users. Data is sold to multiple advertisers at the same time. Currently, there is no way to see how many advertisers use the same data at a certain point of time. So, there are two cost elements to be taken into account; a) using the targeting data incurred a cost (cost per mille increase) b) as multiple advertisers try to reach the same user at the same time the bidding competition increases which drives up prices for the ad-space. While the price for data is visible for the advertiser upfront the increase in competition and subsequently inventory prices (price for the ad space) is only measurable throughout the campaign. As a consequence, when selecting data providers and their segments advertisers should strive to understand how often the data is used by other parties. 6.2.2 A crude metric to evaluate targeting data efficiency Given one has approximated in target (data quality) figures for each data provider, one can calculate the efficiency gains. Here is a simplified example. Let’s assume we want to reach a female target group and we have some initial data quality metrics for three publishers. Also, we know their pricing for the impression (Cost per mille) and their targeting cost. Adding these two up brings us to the total CPM. Dividing the total CPM by the percentage on target (data quality) gives us the effective CPM. Publisher % on target (data quality) CPM - impression CPM - targeting data Total CPM Effective CPM A (no targeting) 50% 5 0 5 10.00 B 75% 6 1 7 9.33 C 85% 6 2 8 9.41 We can easily see that publisher B is the most effective with the lowest cost to reach the target group. As a guidance; the smaller the target group, the more important targeting data is. Note that effective CPM is a very crude measure. It basically assumes that impressions that do no reach the target group are basically worthless. This is very rarely the case. For jewelry the targeting might focus on a female audience, however reaching a male audience who might buy necklace for their partner is not worthless. In is important to keep in mind that buying audience/targeting data is a tool to improve marketing effectiveness. In itself it does transport the marketing message. In the trade-press and sales pitches the importance is of targeting data is often inflated. 6.2.3 The data provider dilemma In the previous example we also see that targeting data is charged on a cost per mille basis. The more impressions/users are reached the higher the total revenue for the data provider. This inserts a problematic relationship for each data provider between data quality and reach. In general, improving data quality increases cost for the data provider in terms of technology, data integrations, better algorithms, better calibration etc. Providing more reach on the other hand increases the data provider’s profit. As a recap of the previous chapter, data providers have to set prediction threshold. The threshold describes which probability is enough to classify users in certain segments. Increasing the threshold increases the prediction accuracy. However, setting the threshold higher, reduces the number of users classified and the data provider’s reach. Subsequently, increasing reach without sacrificing quality is very costly or simply impossible. 6.3 Evaluating targeting data After running advertising campaigns, the usual process requires marketeers to evaluate the campaign’s targeting in order to run the upcoming campaign more efficiently. Oftentimes the results are mixed and the root causes of the mixed results can only rarely be pinned to the data quality of individual providers. Given that the creative and messaging worked well, there are at least two aspects that are always interrelated: the idea or translation of the targeting and the data quality. While there are simple cases for which data is readily available (gender or age targeting), especially for niche campaigns there is no data provider offering the data or the reach is too limited. In these cases, marketeers work with proxy targeting approaches. They translate the original targeting with one for which data is available with enough reach. To give an example; for a tire campaign (a car tire for increased road security) the campaign might use a targeting on families with infants with the idea that fresh families have an increased desire for higher road safety. As both targeting settings “in-market for a car tire with high safety features” as well as “young families” might not be readily available, the marketeer uses the proxy “interest in baby buggies”. While it is easy to follow the reasoning and problem arises when evaluating the results. The following chart illustrates the trouble. Root causes leading to mediocre results The root cause cannot be identified. Hence in situations where the advertiser uses a proxy targeting the data quality cannot easily be observed. In cases where the advertiser does not use proxy targeting there two approaches to measure data quality. The following chart gives the overview. Approaches to measure data quality For awareness campaigns (using demographic or interest data) the most common approach is to use a measurement panel. Some users who have been exposed to a campaign are surveyed regarding their interest and demographic. Based on the survey data the marketeer can calculate the share of on target. There are two major drawbacks to this approach; a) it is costly to conduct b) the on-target measure can usually only be calculated for the whole campaign. So, if an advertiser uses multiple data providers their results will be pooled. The reason for that being that in order to run a panel approach to distinguish between require massive panel sizes.4 The second approach uses so-call performance metrics such as Cost per Conversion or Cost per Order. This approach is mostly used for businesses where users can perform the buying process (or a related action) on the advertiser’s website. The following chapter describes the pitfalls of the current process. What advertisers need to consider when buying targeting data from third parties: How can data provider claims be validated before a campaign? What is the right process to evaluate and select data providers? How is targeting data quality measured during the campaign? How is the ROI of using targeting data quantified? Does it make sense to use the same data as your competition? see: http://www.adnews.com.au/news/facebook-says-platform-can-reach-1-7m-more-young-adult-users-than-aussie-population↩ see: http://flovv.github.io/Panel-Sizes/↩ "],
["measuring-ad-effectiveness.html", "Chapter 7 Measuring Ad Effectiveness 7.1 Current state of digital measurement 7.2 Incremental ad effectiveness 7.3 The attribution discussion - a smoke screen", " Chapter 7 Measuring Ad Effectiveness In order to improve marketing activities, companies use specific user segments to show them advertisements fitting the user’s interest. To be more concrete; users visiting car sites are more likely to see BWM or Daimler-Benz ads. 7.1 Current state of digital measurement So far this sounds great. It is a win-win situation, advertisers reach an audience with an interest in their product and users see ads that mirror their interest. Well the key is; advertisers want to increase their advertising effectiveness. Let’s have a look at how advertising is measured. Let’s exemplify it by looking at the advertising for a car with two segments; a segment of car enthusiasts and a general population segment. Both segments receive the same ad. An example of two user segments The most common (and maybe intuitive) way to look at advertising effectiveness is by calculating the cost per order/conversion. The calculation is straight-forward. The cost per order is: the ad spend by user segment divided by the number of orders generated in the user segment. As segments are generated depending on the user interests, it is fair to assume that users have different base probabilities of buying the item (car) in question. User segment #User Base probability Ad uplift Ad Spend #Orders Cost per order Car enthusiast 100 1.5% 0.50%pp 10 000 € 2 5 000 € No Targeting 100 1.0% 0.50%pp 10 000 € 1.5 6 667 € Given the example, a naive marketeer concludes that targeting the car enthusiast segment is more effective as the cost per order is 1 667€ lower. However, that is only the easy way of calculating it. What you as a marketeer really want to measure is the incremental uplift. The uplift describes the increase in purchase probability when showing the user ads. The incremental uplift is only measurable by running (controlled) experiments, in which you measure the increase of sales in the segment exposed to ads to the “same” segment not exposed to ads. Such controlled experiments are rarely done on a segment basis. In this example, both marketing activities have the same incremental uplift in purchase probability (0.5%pp). Hence, if you ignore that the marketeer has to buy targeting data on top of her media spend, the incremental uplift is the same for both groups as is the return for the ad investment. This brings us to the key message; current systems (ad serving in particular) are designed in a way that they are basically unable to capture users base purchase probability. Hence, they are deeply flawed and purposefully biased towards benefiting targeted ad buying. 7.2 Incremental ad effectiveness In the last chapter, we discussed how the current digital measurement approach is biased towards targeted ad buying. The key reason is that ad effectiveness is calculated on a cost per order/conversion basis. As particular user segments -which are addressed with digital targeting- have a high base purchase probability, the segment looks more responsive. Due to insufficient experimentation and proper measurement, the budget optimization tries to reach users who are likely to order/“convert” anyway. Hence, it ignores incremental effects and instead focuses on overall purchase probabilities, mixing base purchase probability and ad-driven incremental effects. Let’s go back a step and have a look at two scenarios. Scenario A is taken from the previous post. There are two user segments; a segment of car enthusiasts and a general population segment. Both segments receive the same ad. The car enthusiasts (Segment 1) has a higher base probability of purchasing a car. As discussed, that leads to a lower cost per order for that group and sub-sequentially to a misguided budget shift. Example of ad effectiveness Let’s add the second Scenario B. In this case the incremental effect of showing an ad is higher in the general population segment (Segment 2) compared so Segment 1. It might be that the ad communicates a new feature (Matrix LED lights, head-up-display,…) that Segment 1 was not aware of before. In that scenario a budget shift would increase the overall sales. However, that budget shift would only be realized if the incremental effect would have been measured. The cost per order is still lower for Segment 1. To sum up, current systems try hard to minimize the cost per conversion. This leads to showing ads to users who are likely to convert anyway, if possible. From a theoretical point of view, (e.g. assuming that targeting works perfectly) that leads to an overall decreasing ad effectiveness. 7.2.1 Ad effectiveness a Google Adwords example In the last two chapters, we discussed how measurement and false metrics drive optimization towards low hanging fruits and in the end degrade ad effectiveness. The next paragraph details short example of how the issue extends into the paid search (e.g. Google Adwords) channel. Search traffic is split up into two parts; the organic, free traffic and paid traffic. The following chart illustrates the difference. Paid and organic search traffic For the paid part advertisers are allowed buy individual keywords (search terms that users might type in) on a cost per click basis. Each keyword sells for a different cost per click depending on competition, relevancy and click-through-rate. Usually the advertiser picks the keywords with lowest cost per click. Keywords can be clustered into at least two broad categories: Brand and Generic keywords. Brand keywords contain the brand name of the advertiser while generic keywords do not and just relate to an (unspecific) inquiry. From a consumer journey perspective, generic terms are searched in the beginning of a purchase decision process, while brand keywords are used towards the end. Generally, that means that brand keywords have a much lower cost per click than generic keywords. Hence advertisers buy brand keywords all the time. 7.2.2 The digital advertising industry avoids discussing the increment It is safe to assume that advertisers are organically listed in the top positions for brand keywords, while they might not even appear on the first results page for generic terms. As paid advertising is shown above the organic section a majority of users click on the paid links. Hence, an advertiser buying her “own” brand keywords is cannibalizing her organic, free traffic with paid search traffic. The following chart illustrates this: Example of brand and generic keyword buying The share of incremental traffic for buying generic keywords is much higher compared to buying brand keywords. Using proper experimentation one can figure out what that share is and calculate the cost per incremental visitor as a more reliable metric. The question arises why this is usually not done. I see three main reasons: (i) incentives, (ii) additional work and (iii) Google’s quality score. For the team managing paid search it is much easier to communicate low costs per click (CPC) readily delivered by the system compared to relying on a derived metric that needs constant experimentation. As most people are familiar with CPCs they are also commonly used to benchmark channels and/or team performance. Hence increasing CPCs is a difficult story to tell. A part of the cost per click on a keyword is determined by the quality score, Google assigns to an individual advertiser. The lower the score the higher the cost an advertiser has to pay per click. Even though Google does not openly discuss the algorithm behind the quality score5, it is pretty clear that the Click-through-Rate (which is higher for brand keywords) improves the score. As a result of their policy brands are forced or at least given a decent incentive to buy their brand keywords. To sum up, in the paid search channel, the system is designed in a way to ensure that brands buy keywords for users who are very likely to click/convert anyways. 7.3 The attribution discussion - a smoke screen In contrast to facing the challenge of measuring incremental ad effectiveness, the dominating advertising technology companies have built a smoke screen labeled attribution. Attribution modelling is the process of identifying a set of advertisements that influence individuals to engage in a desired behavior (referred to as conversion), and then assigning a value to each of these advertisements. The goal is to provide a level of understanding of which ads, in which channels, what combination of ad displays, and in what particular order work best to influence individuals to buy. As we cannot influence particular individuals but rather increase purchase probability, the key question can be re-phrased as: how does an advertiser increase the conversion (or purchase) probability? In order to evaluate touchpoints/media channels, we want to measure the incremental (additional) purchase probability of showing a specific ad. The so-called rule-based model attribution models dominate the current discussion. Most clients agree that a last click attribution model (the last click/interaction gets all the credit) does not fully describe how users behave and is biased towards lower funnel activities (remarketing, affiliate, brand keywords). As an alternative, clients might also discuss and arbitrarily choose one of the following rule-based models. Commonly discussed attribution models - credit to Google The key problem with all rule-based approaches is that they do not measure incrementality but distribute all (digital) conversions between the digital channels. As a consequence, digital channels take credit for all offline (non-measured) activities as well as the conversions that would have happened anyways. Digital data trails allow profiling users according to their individual interest and intent. If users are correctly profiled, one needs to assume that their propensity to buy is higher compared to the baseline. Current attribution approaches do not take a user’s base probability to purchase into consideration. Hence, the attribution mixes base probability and the increase due to showing ads. In order to properly model it, we need to compare users who converted to the non-converters. Ideally the users are basically identical in terms of demographics, interest, intent, etc. However, current ad serving systems do not measure users who do not convert. Another alternative work-around to this issue is to add “no-show” or “blank” ad impressions to the mix in order to get the uplift per ad impression. In the one setting users see an ad (group A). In the other setting, similarly targeted users see a blank ad impression (group B). The difference in the purchase probability between the A/B group is the incremental effect the ad had. As a conclusion: Attribution is the industry response to avoid discussing incremental ad measurement. Advertisers need to either invest in alternative measurement systems and/or implement proper A/B-test settings to distinguish between base probability and ad effectiveness. Questions that advertisers need to ask: Do I need to measure incrementality? How can I measure incrementality especially in digital channels? How can I can I compare advertising performance between digital and offline channels? What are the right optimization metrics for my digital channels? https://support.google.com/adwords/answer/2454010?hl=en↩ "],
["conclusions.html", "Chapter 8 Conclusions 8.1 Data: A market for lemons 8.2 Ad effectiveness: The industry is not striving for better measurement 8.3 Outlook", " Chapter 8 Conclusions There are two key message that one can take from this book. First, the market for targeting data is a race to the bottom. Second, it is notoriously hard to measure the incremental sales effect of digital advertising. 8.1 Data: A market for lemons In 1970 the the Nobel prize winner G. Akerlof described a market situation in which buyers can expect only subpar products (so-call lemons) to be offered. The market for data market is a perfect example for a lemon market. The Akerlof’s idea is that if buyers of goods cannot distinguish between high- and low-quality goods/providers, one can expect providers of high-quality goods to leave the market. As a consequence, buyers can expect only low quality (lemon) providers to be active. This is exactly the case in the data market. As discussed above, advertisers have no way to validate data quality before running a campaign. Even if they run a campaign, measuring the incremental effect of targeting is in most cases very hard and costly. To make matters worse, there is no open rating/feedback system for data providers. If advertisers have had performance issues with data providers, they will keep the information internally in order to not help their competition. Finally, as advertisers run a limited number of campaigns per year the learning about data quality takes at least several years. From the data provider perspective, maintaining data quality is opposing to increasing reach and subsequently profit. When pressured by shareholders and investors, data provider will always lower the classification threshold in order to increase reach. As a decision maker within a data provider, it is very clear that one needs to invest in a sales pitch (including artificial intelligence buzzwords) first before investing in higher data quality. To sum up, all economic incentives drive data quality towards the bottom. 8.2 Ad effectiveness: The industry is not striving for better measurement In a 2018 article the world leading business consultancy states: “In recent years, the proliferation of technologies that can process massive data sets, combined with the growth of digital advertising channels—which are inherently more measurable—has unlocked a massive opportunity to measure the performance of marketing investments. 6 It is easy to get caught in the number of digital metrics and the promise of better measurement. However, these claims are just wrong. As discussed in the previous chapter, digital targeting and measurement routinely ignores the fact that users have a base probability to buy a product or service. Instead of measuring the incremental number of converted users it takes the total number of converted users as denominator. Hence the digital ROI is often massively inflated. When confronted with measurement approaches, the industry response is a smoke screen called attribution. This set of methods simply distributes all digitally measured conversions (excluding direct type-ins) between the involved digital channels. For a traditionally channel like TV this would mean that all offline sales of persons who happen to watch a TV commercial in the last 30 days are driven by the commercial. In contrast to digital, it is the norm for offline channels to measure ad effectiveness as an incremental effect to baseline sales using market mix modelling. This incremental measurement is rarely done in digital channels and most industry participants work hard to distract advertisers to measure the true ROI. 8.3 Outlook All in all, the value of targeting data for society is very limited. The massive tracking systems are used for a marketing machine which cannot accurately measure the incremental effect of the collected data. Rather, the data collection is used as foundation to argue for the superiority of digital channels over traditional marketing methods. On the flipside, the public is increasing aware and skeptical of the massive data collection. Public bodies like the EU started to increase regulation. The General Data Protection Regulation (GDPR) is the most important change in data privacy regulation in 20 years. GDPR substantially reduced the extent to which targeting data can be shared, processed and stored. Without explicit consent, data providers are not allowed to collect data on users. Finally, some technology providers realized that data privacy can be a differentiator. Intelligent tracking prevention (Apple, Firefox, ad-blocking) stops 3rd party pixels from firing in the first place. Hence, by default user groups with these devices and browsers are not tracked and cannot be profiled with the current technology stack. https://www.mckinsey.com/business-functions/operations/our-insights/zero-based-productivity-marketing-measure-allocate-and-invest-marketing-dollars-more-effectively↩ "]
]
